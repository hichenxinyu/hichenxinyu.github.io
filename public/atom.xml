<?xml version="1.0" encoding="utf-8"?>


<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN">
    <title type="text">Nevea&#39;s Blog</title>
    <subtitle type="html">MemE 是一个强大且可高度定制的 GoHugo 博客主题，专为个人博客设计。</subtitle>
    <updated>2020-04-05T01:12:11+08:00</updated>
    <id>https://example.com/</id>
    <link rel="alternate" type="text/html" href="https://example.com/" />
    <link rel="self" type="application/atom+xml" href="https://example.com/atom.xml" />
    <author>
            <name>陈欣宇</name>
            <uri>https://example.com/</uri>
            
                <email>hichenxinyu@gmail.com</email>
            </author>
    <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    <generator uri="https://gohugo.io/" version="0.62.0">Hugo</generator>
        <entry>
            <title type="text">我常用的 Mac 软件</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/mac%E8%BD%AF%E4%BB%B6/" />
            <id>https://example.com/posts/mac%E8%BD%AF%E4%BB%B6/</id>
            <updated>2020-04-04T23:59:19+08:00</updated>
            <published>2020-03-23T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[选择软件的一些标准 跨平台 开源免费 简洁 无广告 Markdown编辑器：Typora+Pi……]]></summary>
            
                <content type="html"><![CDATA[<p>选择软件的一些标准</p>
<ul>
<li>跨平台</li>
<li>开源免费</li>
<li>简洁 无广告</li>
</ul>
<hr>
<p>Markdown编辑器：Typora+PicGo
​	 开源免费的，所见即所得的实时预览很舒服，界面也非常清爽，而且还是最近也支持了PicGo，配合腾讯云的COS来做图床，</p>
<p>编辑器 ：VScode
​    日常写脚本，写个Python代码，丰富的插件，</p>
<p>视频播放器： IINA</p>
<p>解压缩软件：The Unarachiver</p>
<p>云盘： OneDrive ，iCloud
OneDrive是Office 365 订阅赠送，日常来用作资料同步 ，Apple 全家桶的前提下，iCloud非常好用，免费的5GB也够用了，大文件放在Onedrive中。</p>
<p>Git GUI客户端：GitKraken
​	 非常经典的章鱼，感觉比Sourcetree好用，基于electron ，跨平台。免费版本也够用了，只是不支持github私有仓库。</p>
<p>终端：iTerm2
​	iTerm2 +oh-my-zsh ，作为运维，命令行是经常打交道的地方，一个好看好用功能强大的终端是以一个很好的工具，Mac上最好用的终端工具</p>
<p>快捷工具：AIfred3
​	日常搜索文件，搜索软件速度更快</p>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Kubernetes集群安装</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/kubernetes-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/" />
            <id>https://example.com/posts/kubernetes-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</id>
            <updated>2020-04-04T23:59:16+08:00</updated>
            <published>2019-07-20T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[Master包含组件 Master 组件提供的集群控制，Master 组件对集群做出全局性决策(例如……]]></summary>
            
                <content type="html"><![CDATA[<h3 id="master">Master包含组件</h3>
<p>Master 组件提供的集群控制，Master 组件对集群做出全局性决策(例如：调度)等，负责维护集群的目标状态。</p>
<table>
<thead>
<tr>
<th>组件</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>etcd</td>
<td>用于 Kubernetes 的后端数据存储，所有集群数据都存储在此处</td>
</tr>
<tr>
<td>kube-apiserver</td>
<td>对外暴露了 Kubernetes API，它是的 Kubernetes 前端控制层，只有 API Server 会与 etcd 通信，其它模块都必须通过 API Server 访问集群状态。</td>
</tr>
<tr>
<td>kube-controller-manager</td>
<td>处理集群中常规任务，它是单独的进程，内部包含多个控制器，包含节点控制器,副本控制器,端点控制器,服务帐户和令牌控制器</td>
</tr>
<tr>
<td>kube-scheduler</td>
<td>监视新创建的 Pod 为新创建的 POD 分配合适的 node 节点</td>
</tr>
<tr>
<td>addons（插件）</td>
<td>插件是实现集群功能的 Pod 和 Service，一般被创建于 kube-system 命名空间。例如：coreDNS</td>
</tr>
</tbody>
</table>
<h3 id="node--">Node  包含组件</h3>
<p>Node 节点实际负责实施，也就是运行 POD 的节点，上面运行的组件有</p>
<table>
<thead>
<tr>
<th>组件</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>kubelet</td>
<td>它监测已经分配给自己的 Pod，为 POD 准备卷，下载 POD 所需的 Secret，下载镜像并运行，进行生命周期探测，上报 POD 和节点状态</td>
</tr>
<tr>
<td>kube-proxy</td>
<td>通过维护主机上的网络规则并执行连接转发，将 Kubernetes 提供的网络服务代理到每个节点上，实现了Kubernetes服务抽象</td>
</tr>
<tr>
<td>docker</td>
<td>用于运行容器</td>
</tr>
</tbody>
</table>
<pre><code>#!/bin/bash
#执行该脚本需要添加k8s版本号,作为参数 将脚本第79行（已高亮）的 ${1} 替换成您需要的版本号，例如 1.17.0
# 在 master 节点和 worker 节点都要执行

# 安装 docker
# 参考文档如下
# https://docs.docker.com/install/linux/docker-ce/centos/ 
# https://docs.docker.com/install/linux/linux-postinstall/

# 卸载旧版本
yum remove -y docker \
docker-client \
docker-client-latest \
docker-common \
docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-selinux \
docker-engine-selinux \
docker-engine

# 设置 yum repository
yum install -y yum-utils \
device-mapper-persistent-data \
lvm2
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

# 安装并启动 docker
yum install -y docker-ce-18.09.7 docker-ce-cli-18.09.7 containerd.io
systemctl enable docker
systemctl start docker

# 安装 nfs-utils
# 必须先安装 nfs-utils 才能挂载 nfs 网络存储
yum install -y nfs-utils
yum install -y wget

# 关闭 防火墙
systemctl stop firewalld
systemctl disable firewalld

# 关闭 SeLinux
setenforce 0
sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config

# 关闭 swap
swapoff -a
yes | cp /etc/fstab /etc/fstab_bak
cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab

# 修改 /etc/sysctl.conf
# 如果有配置，则修改
sed -i &quot;s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward=1#g&quot;  /etc/sysctl.conf
sed -i &quot;s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables=1#g&quot;  /etc/sysctl.conf
sed -i &quot;s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables=1#g&quot;  /etc/sysctl.conf
# 可能没有，追加
echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf
echo &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.conf
echo &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.conf
# 执行命令以应用
sysctl -p

# 配置K8S的yum源
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 卸载旧版本
yum remove -y kubelet kubeadm kubectl

# 安装kubelet、kubeadm、kubectl
yum install -y kubelet-${1} kubeadm-${1} kubectl-${1}

# 修改docker Cgroup Driver为systemd
# # 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
# # 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd
# 如果不修改，在添加 worker 节点时可能会碰到如下错误
# [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. 
# Please follow the guide at https://kubernetes.io/docs/setup/cri/
sed -i &quot;s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g&quot; /usr/lib/systemd/system/docker.service

# 设置 docker 镜像，提高 docker 镜像下载速度和稳定性
# 如果您访问 https://hub.docker.io 速度非常稳定，亦可以跳过这个步骤
curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io

# 重启 docker，并启动 kubelet
systemctl daemon-reload
systemctl restart docker
systemctl enable kubelet &amp;&amp; systemctl start kubelet

docker version
</code></pre><p><a name="D3lWQ"></a></p>
<h3 id="-master-">初始化 Master 节点</h3>
<pre><code># 只在 master 节点执行
# 替换 x.x.x.x 为 master 节点的内网IP
# export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令
export MASTER_IP=x.x.x.x
# 替换 apiserver.demo 为 您想要的 dnsName
export APISERVER_NAME=apiserver.demo
# Kubernetes 容器组所在的网段，该网段安装完成后，由 kubernetes 创建，事先并不存在于您的物理网络中
export POD_SUBNET=10.100.0.1/16
echo &quot;${MASTER_IP}    ${APISERVER_NAME}&quot; &gt;&gt; /etc/hosts
</code></pre><h3 id="21-1--1170">请将脚本第21行（已高亮）的 ${1} 替换成您需要的版本号，例如 1.17.0</h3>
<pre><code>#!/bin/bash

# 只在 master 节点执行

# 脚本出错时终止执行
set -e

if [ ${#POD_SUBNET} -eq 0 ] || [ ${#APISERVER_NAME} -eq 0 ]; then
  echo -e &quot;\033[31;1m请确保您已经设置了环境变量 POD_SUBNET 和 APISERVER_NAME \033[0m&quot;
  echo 当前POD_SUBNET=$POD_SUBNET
  echo 当前APISERVER_NAME=$APISERVER_NAME
  exit 1
fi


# 查看完整配置选项 https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2
rm -f ./kubeadm-config.yaml
cat &lt;&lt;EOF &gt; ./kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v${1}
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
controlPlaneEndpoint: &quot;${APISERVER_NAME}:6443&quot;
networking:
  serviceSubnet: &quot;10.96.0.0/16&quot;
  podSubnet: &quot;${POD_SUBNET}&quot;
  dnsDomain: &quot;cluster.local&quot;
EOF

# kubeadm init
# 根据您服务器网速的情况，您需要等候 3 - 10 分钟
kubeadm init --config=kubeadm-config.yaml --upload-certs

# 配置 kubectl
rm -rf /root/.kube/
mkdir /root/.kube/
cp -i /etc/kubernetes/admin.conf /root/.kube/config

# 安装 calico 网络插件
# 参考文档 https://docs.projectcalico.org/v3.10/getting-started/kubernetes/
echo &quot;安装calico-3.10.2&quot;
rm -f calico-3.10.2.yaml
wget https://kuboard.cn/install-script/calico/calico-3.10.2.yaml
sed -i &quot;s#192\.168\.0\.0/16#${POD_SUBNET}#&quot; calico-3.10.2.yaml
kubectl apply -f calico-3.10.2.yaml
</code></pre><p><a name="AnGTK"></a></p>
<h4 id="master-">检查Master 初始化结果</h4>
<pre><code># 只在 master 节点执行

# 执行如下命令，等待 3-10 分钟，直到所有的容器组处于 Running 状态
watch kubectl get pod -n kube-system -o wide

# 查看 master 节点初始化结果
kubectl get nodes -o wide
</code></pre><p><a name="9imZK"></a></p>
<h3 id="--work-">初始化  work 节点</h3>
<pre><code># 只在 master 节点执行
kubeadm token create --print-join-command
</code></pre><pre><code># 只在 worker 节点执行
# 替换 x.x.x.x 为 master 节点的内网 IP
export MASTER_IP=x.x.x.x
# 替换 apiserver.demo 为初始化 master 节点时所使用的 APISERVER_NAME
export APISERVER_NAME=apiserver.demo
echo &quot;${MASTER_IP}    ${APISERVER_NAME}&quot; &gt;&gt; /etc/hosts

# 替换为 master 节点上 kubeadm token create 命令的输出
kubeadm join apiserver.demo:6443 --token mpfjma.4vjjg8flqihor4vt     --discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303
</code></pre><p><a name="lmnJc"></a></p>
<h4 id="heading">检查初始化结果</h4>
<pre><code># 只在 master 节点执行
kubectl get nodes -o wide
</code></pre><p>输出结果如下所示</p>
<pre><code>[root@demo-master-a-1 ~]# kubectl get nodes
NAME     STATUS   ROLES    AGE     VERSION
demo-master-a-1   Ready    master   5m3s    v1.17.x
demo-worker-a-1   Ready    &lt;none&gt;   2m26s   v1.17.x
demo-worker-a-2   Ready    &lt;none&gt;   3m56s   v1.17.x
</code></pre>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Kafka & ZooKeeper 集群搭建</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/kafka-zookeeper%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/" />
            <id>https://example.com/posts/kafka-zookeeper%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</id>
            <updated>2020-04-04T23:59:13+08:00</updated>
            <published>2019-07-11T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[环境准备 三台机器 CentOS 7.4 10.0.19.218 10.0.19.216 10.0.19.142 kafka 集群搭建准备: Java环境 ZooKeeper 集群搭建 Zookeeper 集群搭建 server.1=10.0.19.218:2888:3888 server.2=10.0.19.216:2888:3888 server.3=10.0.19.142:2888:3888 echo &quot;1&quot;……]]></summary>
            
                <content type="html"><![CDATA[<h4 id="heading">环境准备</h4>
<p>三台机器 CentOS 7.4
10.0.19.218
10.0.19.216
10.0.19.142</p>
<p>kafka 集群搭建准备:</p>
<ul>
<li>Java环境</li>
<li>ZooKeeper 集群搭建</li>
</ul>
<h4 id="zookeeper-">Zookeeper 集群搭建</h4>
<pre><code>server.1=10.0.19.218:2888:3888
server.2=10.0.19.216:2888:3888
server.3=10.0.19.142:2888:3888
</code></pre><pre><code>echo &quot;1&quot; &gt;/tmp/zookeeper/myid ##生成ID，这里需要注意， myid对应的zoo.cfg的server.ID，比如第二台zookeeper主机对应的myid应该是2
</code></pre><p>启动ZooKeeper</p>
<pre><code>cd /usr/local/zookeeper-3.4.9/bin
./zkServer.sh start
</code></pre><p>停止Zookeeper</p>
<pre><code>cd /usr/local/zookeeper-3.4.9/bin
./zkServer.sh stop
</code></pre><p>附美味测试环境Zookeeper 集群配置</p>
<pre><code>[root@rdpops_server-146-199 conf]# cat zoo.cfg
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
dataDir=/var/zookeeper/
dataLogDir=/var/zookeeper/datalog/
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
maxClientCnxns=0
#
# Be sure to read the maintenance section of the
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
autopurge.snapRetainCount=30
# Purge task interval in hours
# Set to &quot;0&quot; to disable auto purge feature
#autopurge.purgeInterval=1
server.1=10.0.146.126:2888:3888
server.2=10.0.146.150:2888:3888
server.3=10.0.146.199:2888:3888
</code></pre><hr>
<h4 id="kafka-">Kafka  集群搭建</h4>
<p>下载安装包</p>
<pre><code>wget http://apache.claz.org/kafka/2.2.0/kafka_2.11-2.2.0.tgz
tar -xzf kafka_2.11-2.2.0.tgz
cd kafka_2.11-2.2.0
</code></pre><h4 id="heading-1">启动服务</h4>
<h5 id="kafka--zookeeper">kafka 依赖 zookeeper</h5>
<h5 id="-kafka-server">启动 Kafka server:</h5>
<pre><code>./kafka-server-start.sh -daemon ../config/server.properties 
</code></pre><h4 id="kafka-web-kafka-manager">Kafka Web管理工具-Kafka-manager</h4>
<h4 id="-">消息生产 测试</h4>
<pre><code>./kafka-console-producer.sh --broker-list localhost:9092 --topic demo
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic demo
</code></pre>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Redis 持久化</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/redis-%E6%8C%81%E4%B9%85%E5%8C%96/" />
            <id>https://example.com/posts/redis-%E6%8C%81%E4%B9%85%E5%8C%96/</id>
            <updated>2020-04-04T23:59:29+08:00</updated>
            <published>2019-07-11T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[Redis的读写性能俱佳，但由于是内存数据库，如果没有提前备份，Redis数据是掉电……]]></summary>
            
                <content type="html"><![CDATA[<p>Redis的读写性能俱佳，但由于是内存数据库，如果没有提前备份，Redis数据是掉电即失的。
Redis提供了两种方式进行持久化：</p>
<ol>
<li>RDB持久化</li>
<li>AOF持久化</li>
</ol>
<h3 id="heading">原理</h3>
<p>将Redis在内存中的数据定时<code>dump</code>到磁盘上，实际操作过程是<code>fork</code>一个子进程，先将数据写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储
打印rdb文件</p>
<pre><code>root@pa6:/var/lib/redis# od -c dump.rdb
0000000   R   E   D   I   S   0   0   0   6 376  \0  \0 003   k   e   y
0000020 301 177   # 377   ]   } 362 366 313 362   n 020
0000034
</code></pre><p><img src="https://blog-pic-1253367462.cos.ap-shanghai.myqcloud.com/1567665181186-fcf56c20-aed6-4cb4-b55b-edeff6d6c3a3.png" alt="image.png"></p>
<h4 id="aof-">AOF 持久化存储</h4>
<p>AOF持久化：将Redis的操作日志以文件追加的方式写入文件，只记录写、删除操作，查询操作不会记录（类似于MySQL的Binlog日志）</p>
<p><img src="https://blog-pic-1253367462.cos.ap-shanghai.myqcloud.com/1567665214953-264e7ec5-2993-431b-9e38-6ccfd3206f57.png" alt="image.png"></p>
<p>因为BGSAVE命令可以在不阻塞服务器进程的情况下执行，所以Redis允许用户通过设置服务器配置的save选项，让服务器每隔一段时间自动执行一次BGSAVE命令
用户可以通过save选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行BGSAVE命令 举个例子，如果我们向服务器提供以下配置
vim redis.conf</p>
<pre><code>save 900 1
save 300 10
save 60 10000
</code></pre><p>那么只要满足以下三个条件中的任意一个，BGSAVE命令就会被执行 服务器在900秒之内，对数据库进行了至少1次修改
服务器在300秒之内，对数据库进行了至少10次修改
服务器在60秒之内，对数据库进行了至少10000次修改。</p>
<h4 id="heading-1">备份脚本</h4>
<pre><code>#!/bin/bash
# 避免造成雪崩
set -e
# bak 目录
BACKUPDIR=/data/redis/backup
#PASSWD='redis密码'
#DATADIR=`/usr/local/bin/redis-cli -p 端口 -a &quot;$PASSWD&quot; config get dir|grep -Ev 'dir|grep'`
# 获取redis-cli 二进制文件
REDIS_CMD=$(which redis-cli)
DATADIR=`$REDIS_CMD  config get dir|grep -Ev 'dir|grep'`
DATE=`date +'%Y-%m-%d-%H-%M'`
BACKUPDIR_DATE=$BACKUPDIR_$DATE

if [ ! -d $BACKUPDIR ];then
        mkdir -p $BACKUPDIR \
        || echo &quot;can't make dir !!!&quot; \
        &amp;&amp; exit 1
fi
#以日期分类
mkdir -p $BACKUPDIR/$BACKUPDIR_DATE
$REDIS_CMD  bgsave
sleep 3
cp -rf $DATADIR/* $BACKUPDIR/$BACKUPDIR_DATE
</code></pre>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Prometheus&#43;Grafana监控实践</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/prometheus/" />
            <id>https://example.com/posts/prometheus/</id>
            <updated>2020-04-04T23:59:26+08:00</updated>
            <published>2019-07-01T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[什么是Prometheus Prometheus 是由 SoundCloud 开源监控告警解决方案，从 2012 年开始编写代码，再到 2015 年……]]></summary>
            
                <content type="html"><![CDATA[<h4 id="prometheus">什么是Prometheus</h4>
<blockquote>
<p>Prometheus 是由 SoundCloud 开源监控告警解决方案，从 2012 年开始编写代码，再到 2015 年 github 上开源以来，已经吸引了 9k+ 关注，以及很多大公司的使用；2016 年 Prometheus 成为继 k8s 后，第二名 CNCF(Cloud Native Computing Foundation) 成员。</p>
</blockquote>
<p>作为新一代开源解决方案，很多理念与 Google SRE 运维之道不谋而合。</p>
<h4 id="prometheus-1">为什么选择prometheus：</h4>
<ul>
<li>Prometheus 是按照 Google SRE 运维之道的理念构建的，具有实用性和前瞻性。</li>
<li>Prometheus 社区非常活跃，基本稳定在 1个月1个版本的迭代速度，从 2016 年 v1.01 开始接触使用以来，到目前发布的 v1.8.2 以及最新最新的 v2.1 ，你会发现 Prometheus 一直在进步、在优化。</li>
<li>Go 语言开发，性能不错，安装部署简单，多平台部署兼容性好。</li>
<li>丰富的数据收集客户端，官方提供了各种常用 exporter。</li>
<li>丰富强大的查询能力,内置更强大的统计函数。</li>
<li>Prometheus 属于一站式监控告警平台，依赖少，功能齐全。</li>
<li>Prometheus支持对云或容器的监控，其他监控系统主要对主机监控。</li>
</ul>
<!-- more -->
<p>Prometheus 在数据存储扩展性以及持久性上没有 InfluxDB，OpenTSDB，Sensu 好。</p>
<h4 id="prometheus-2">安装prometheus</h4>
<ol>
<li>下载二进制包</li>
</ol>
<pre><code>在这个页面https://prometheus.io/download
找到最新的二进制包
wget https://github.com/prometheus/prometheus/releases/download/v2.7.2/prometheus-2.7.2.linux-amd64.tar.gz
</code></pre><ol start="2">
<li>解压并进入对应文件夹</li>
</ol>
<pre><code>tar xvfz prometheus-*.tar.gz
cd prometheus-*
</code></pre><ol start="3">
<li>启动prometheus</li>
</ol>
<pre><code>./prometheus --config.file=prometheus.yml
</code></pre><p>grafana dashboard
主机基础监控 dashboard id 9276</p>
<h5 id="prometheus--9090address00009090-ip9090--grafana">此时，prometheus 服务已在后台运行，注意暴露的端口号 9090（address=0.0.0.0:9090），可以直接在浏览器打开 ip:9090 查看简易控制台，此控制台可以做调试使用，需要更强大、美观的数据展示官方建议使用 Grafana.</h5>
<hr>
<h4 id="-grafana">安装 grafana</h4>
<p>（数据展示<a href="https://grafana.com">https://grafana.com</a>）</p>
<pre><code>wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.0.2-1.x86_64.rpm

yum  localinstall grafana-5.0.2-1.x86_64.rpm

service grafana-server start
</code></pre><p>此时，grafana 服务已经启动，可以直接在浏览器中打开 localhost:3000 开始体验 grafana 了</p>
<hr>
<h4 id="exporter">exporter</h4>
<p>Prometheus官网有很多exporter，exporter可以理解为<strong>采集客户端</strong>，即采集指标，通过http方式暴露metrics给prometheus，Prometheus通过pull方式拉取这些指标。</p>
<ol>
<li>node_exporter（linux 系统信息采集器）</li>
</ol>
<pre><code>wget https://github.com/prometheus/node_exporter/releases/download/v0.18.0/node_exporter-0.18.0.linux-amd64.tar.gz

tar -zxvf node_exporter-0.18.0.linux-amd64.tar.gz

cd node_exporter-0.18.0.linux-amd64

nohup ./node_exporter &amp; （在后台运行系统信息采集器）
</code></pre><p>可以通过curl查看<br>
curl 127.0.0.1:9100/metrics</p>
<blockquote>
<p>Tips：0.16.0 版本在 centos 6.x 运行时报错 kernel too old，下载 0.15.2 或之前版本即可
<a href="https://github.com/prometheus/node_exporter/releases/download/v0.15.2/node_exporter-0.15.2.linux-amd64.tar.gz">https://github.com/prometheus/node_exporter/releases/download/v0.15.2/node_exporter-0.15.2.linux-amd64.tar.gz</a></p>
</blockquote>
<p>此时，系统信息采集器已在后台运行，等待 prometheus 服务来拉取数据，注意暴露的端口号 9100（msg=&quot;Listening on :9100&quot;），prometheus 拉取数据需要使用此端口号</p>
<h4 id="-prometheus">安装 prometheus</h4>
<p>prometheus <a href="https://prometheus.io/">https://prometheus.io/</a>）</p>
<pre><code>wget https://github.com/prometheus/prometheus/releases/download/v2.2.1/prometheus-2.2.1.linux-amd64.tar.gz
tar -zxvf prometheus-2.2.1.linux-amd64.tar.gz
cd prometheus-2.2.1.linux-amd64
</code></pre><p>vi prometheus.yml</p>
<pre><code>global:
  scrape_interval: 10s
  evaluation_interval: 10s

scrape_configs:
  - job_name: prometheus
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:9090']
        labels:
          instance: prometheus

  - job_name: mysql
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:9104']
        labels:
          instance: mysql

  - job_name: 'linux'
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:9100']
        labels:
          instance: linux
</code></pre><h5 id="heading">启动</h5>
<p>nohup ./prometheus --config.file=&quot;prometheus.yml&quot; &amp;（在后台运行 prometheus 服务）</p>
<h4 id="prometheus-3">Prometheus配置的热加载</h4>
<p>Prometheus配置信息的热加载有两种方式：</p>
<ul>
<li>
<p>第一种热加载方式：查看Prometheus的进程id，发送SIGHUP信号:
kill -HUP <pid></p>
</li>
<li>
<p>第二种热加载方式：发送一个POST请求到/-/reload，需要在启动时给定--web.enable-lifecycle选项：
curl -X POST http://localhost:9090/-/reload</p>
</li>
</ul>
<p>我们使用的是第一种热加载方式，systemd unit文件如下：</p>
<pre><code>[Unit]
Description=prometheus
After=network.target
[Service]
Type=simple
User=prometheus
ExecStart=/usr/local/prometheus/prometheus \
 --config.file=/usr/local/prometheus/prometheus.yml \
 --storage.tsdb.path=/home/prometheus/data \
 --storage.tsdb.retention=365d \
 --web.listen-address=:9090 \
 --web.external-url=https://prometheus.frognew.com
ExecReload=/bin/kill -HUP $MAINPID
Restart=on-failure
[Install]
WantedBy=multi-user.target

</code></pre><p>systemctl reload prometheus</p>
<h4 id="-alertmanager">告警服务 Alertmanager</h4>
<p>告警能力在Prometheus的架构中被划分成两个独立的部分。如下所示，通过在Prometheus中定义AlertRule（告警规则），Prometheus会周期性的对告警规则进行计算，如果满足告警触发条件就会向Alertmanager发送告警信息。
<img src="evernotecid://94C76643-58DA-4407-AFA8-9F2A33EF0AAA/appyinxiangcom/11651347/ENResource/p3081" alt="2957ddc2799240dd958cbdf9bf4a46f7.png"></p>
<h4 id="heading-1">一些问题</h4>
<ol>
<li>为什么选择 pull  而不是push
通过HTTP提供了许多优势：</li>
</ol>
<ul>
<li>您可以在开发更改时在笔记本电脑上运行监控。</li>
<li>您可以更轻松地判断目标是否已关闭。</li>
<li>您可以手动转到目标并使用Web浏览器检查其运行状况。
总体而言，我们认为拉动比推动略好，但在考虑监控系统时，不应将其视为重点。</li>
</ul>
<p>附录:
线上 Prometheus 配置文件 prometheus.yml</p>
<pre><code># my global config
global:
  scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
   - &quot;/etc/prometheus/rules/*.rules&quot;
  # - &quot;second_rules.yml&quot;

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.
  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
    - targets: ['localhost:9090']

  - job_name: 'telegraf'
    file_sd_configs:
      - files: ['./*.json']

remote_write:
  - url: &quot;http://influxdb:8086/api/v1/prom/write?db=prometheus&quot;

remote_read:
  - url: &quot;http://influxdb:8086/api/v1/prom/read?db=prometheus&quot;
</code></pre>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Gitlab CI/CD</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/gitlab-ci/" />
            <id>https://example.com/posts/gitlab-ci/</id>
            <updated>2020-04-04T23:59:08+08:00</updated>
            <published>2019-06-12T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[GitLab CI/CD 是gitlab8.0之后自带的一个持续集成系统，中心思想是当每一次push到gi……]]></summary>
            
                <content type="html"><![CDATA[<p>GitLab CI/CD 是gitlab8.0之后自带的一个持续集成系统，中心思想是当每一次push到gitlab的时候，都会触发一次脚本执行，然后脚本的内容包括了测试，编译，部署等一系列自定义的内容。</p>
<p>GitLab CI/CD的脚本执行，需要自定义安装对应gitlab-runner来执行，代码push之后，webhook检测到代码变化，就会触发gitlab-CI，分配到各个Runner来运行相应的脚本script。这些脚本有的是测试项目用的，有的是部署用的。</p>
<h3 id="gitlab-cicd-">GitLab CI/CD 优势</h3>
<ul>
<li>轻量级，不需要复杂的安装手段。</li>
<li>配置简单，与gitlab可直接适配。</li>
<li>实时构建日志十分清晰，UI交互体验很好</li>
<li>使用 YAML 进行配置，任何人都可以很方便的使用。</li>
</ul>
<h3 id="gitlab-cicd--1">GitLab CI/CD 劣势</h3>
<ul>
<li>没有统一的管理界面，无法统筹管理所有项目</li>
<li>配置依赖于代码仓库，耦合度没有Jenkins低</li>
</ul>
<p>gitlab-runner 安装(个人比较喜欢 yum or rpm 安装软件)</p>
<pre><code>#添加官方 yum 源
curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh | sudo bash
#安装 gitlab-runner 
yum install gitlab-runner -y
</code></pre><p>gitlab-runner 注册 </p>
<pre><code>gitlab-ci-multi-runner register
#然后依次填入  gitlab url，Token，Runner 类型
</code></pre>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">ELK Stack 升级</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/elk-stack-%E5%8D%87%E7%BA%A7%E7%AC%94%E8%AE%B0/" />
            <id>https://example.com/posts/elk-stack-%E5%8D%87%E7%BA%A7%E7%AC%94%E8%AE%B0/</id>
            <updated>2020-04-04T23:58:53+08:00</updated>
            <published>2019-06-11T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[升级方案 比较常见的 ELK Stack 升级方式有一下 3 种 数据迁移升级：新建一套高版本 ES 集群，然后把老……]]></summary>
            
                <content type="html"><![CDATA[<h3 id="heading">升级方案</h3>
<p>比较常见的 ELK Stack 升级方式有一下 3 种</p>
<ul>
<li>数据迁移升级：新建一套高版本 ES 集群，然后把老的 ES数据导入 </li>
<li>滚动升级，5.0.0-&gt;5.6,5.6-&gt;6.8.2</li>
<li>集群重启升级，5.0.0-&gt;6.8.2</li>
</ul>
<table>
<thead>
<tr>
<th>升级方式</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据迁移&amp;升级</td>
<td>可以跨大版本升级</td>
<td>升级过程中，旧集群写操作停止</td>
</tr>
<tr>
<td>ES 版本之间有差异比较大，可能会有一些坑</td>
<td></td>
<td></td>
</tr>
<tr>
<td>迁移时间较长</td>
<td></td>
<td></td>
</tr>
<tr>
<td>滚动升级</td>
<td>升级过程中服务不中断</td>
<td>对ELK 源版本与目标版本有要求</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>集群重启升级</td>
<td>可以跨大版本升级</td>
<td></td>
</tr>
<tr>
<td>迁移时间比较短</td>
<td>ES 服务中断</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>公司生产环境与测试环境的 ELK 技术栈版本是 5.0.0
考虑先滚动升级到 5.6.16  然后确认没问题后，再从 5.6.16 滚动升级到 6.8.2
升级一般很快，但是最麻烦的不是升级，而是保证 ES 中的索引正常。
一般不建议跨大版本升级，而是小版本迭代。</p>
<hr>
<h3 id="heading-1">数据备份</h3>
<p>备份方式：Elasticsearch snapshot
IDC 测试环境可以考虑快照备份到本地 HDFS，磁盘
生产环境 ES 可以快照备份到阿里云 OSS</p>
<blockquote>
<p>需要安装对应的 ES 插件</p>
</blockquote>
<p>本次测试环境 ELK 集群升级，不备份</p>
<h4 id="-kibana--">备份你的 Kibana 数据 （生产环境中非常重要）</h4>
<p>你的 Kibana 中的 index pattern 等数据都是存在 .Kibana 这个索引中的 
具体备份&amp;升级可以参考官方文档
<a href="https://www.elastic.co/guide/en/kibana/6.3/migrating-6.0-index.html">https://www.elastic.co/guide/en/kibana/6.3/migrating-6.0-index.html</a>
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-snapshots.html">https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-snapshots.html</a></p>
<hr>
<h3 id="heading-2">升级前准备</h3>
<p>升级Elasticsearch前，先停止Logstash（hangout）服务，不再消费 kafka 中消息（Filebeat服务无需关闭）</p>
<h4 id="heading-3">禁用集群的分片自动分配功能</h4>
<pre><code>PUT _cluster/settings
{
  &quot;persistent&quot;: {
    &quot;cluster.routing.allocation.enable&quot;: &quot;none&quot;
  }
}
</code></pre><p>启动集群分片自动分配</p>
<pre><code>PUT _cluster/settings
{
  &quot;persistent&quot;: {
    &quot;cluster.routing.allocation.enable&quot;: null
  }
}
</code></pre><h3 id="heading-4">下面这个待确认</h3>
<pre><code>PUT _cluster/settings
{
  &quot;persistent&quot;: {
    &quot;cluster.routing.allocation.enable&quot;: &quot;primaries&quot;
  }
}
</code></pre><h4 id="heading-5">同步数据（强制刷新，尽可能将缓存中的数据，写入磁盘）</h4>
<p>通过 synced flush，可以在索引上放置一个 sync ID，这样可以提高这些分片Recovery的时间</p>
<pre><code>POST _flush/synced
</code></pre><hr>
<h3 id="-es">升级 ES</h3>
<p>注意，这里使用rpm安装时，不使用-Uvh选项升级，而是先卸载旧的版本，再使用-ivh安装。另外，旧的配置文件不建议继续使用，而是将所配置内容替换到新版的配置文件中，除了个别配置外大部分仍然兼容。</p>
<p>Elasticsearch节点，依次执行</p>
<pre><code>#停止当前ES 节点
service elasticsearch stop
#卸载所有 ES 插件  进入/usr/share/elasticsearch目录
#elasticsearch-plugin list
#elasticsearch-plugin remove
# 卸载旧版本注意，卸载后，旧的配置文件会以rpmsave后缀保留
rpm -e elasticsearch
</code></pre><h5 id="heading-6">安装新版本</h5>
<pre><code>rpm -ivh /usr/local/elasticsearch-5.6.16.rpm
</code></pre><h5 id="heading-7">替换配置文件</h5>
<pre><code>cat /etc/elasticsearch/elasticsearch.yml.rpmsave &gt; /etc/elasticsearch/elasticsearch.yml
cat /etc/elasticsearch/jvm.options.rpmsave &gt; /etc/elasticsearch/jvm.options
</code></pre><h5 id="elasticsearch">启动Elasticsearch</h5>
<pre><code>service elasticsearch start
</code></pre><h5 id="heading-8">开启集群的分片自动分配功能</h5>
<p>**</p>
<pre><code>PUT _cluster/settings
{
  &quot;persistent&quot;: {
    &quot;cluster.routing.allocation.enable&quot;: null
  }
}
</code></pre><h5 id="heading-9">等待集群恢复完毕</h5>
<pre><code>GET _cat/health
</code></pre><p>查看集群状态，Green 则恢复正常，升级成功</p>
<hr>
<h3 id="heading-10">验证方式</h3>
<p>API 验证集群状态
Cerebro 中观察分片分配情况</p>
<h3 id="es">升级ES过程中可能会遇到的问题：</h3>
<h5 id="heading-11">启动失败 </h5>
<pre><code>[2019-09-16T20:04:45,116][ERROR][o.e.b.Bootstrap          ] [node-1] node validation exception
[1] bootstrap checks failed
[1]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk
</code></pre><h5 id="etcelasticsearchelasticsearchyml-">/etc/elasticsearch/elasticsearch.yml  添加一行配置</h5>
<pre><code>bootstrap.system_call_filter: false
</code></pre><blockquote>
<p>官方文档见：
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/system-call-filter-check.html">https://www.elastic.co/guide/en/elasticsearch/reference/5.6/system-call-filter-check.html</a></p>
</blockquote>
<hr>
<h3 id="heading-12">升级失败回滚降级</h3>
<pre><code>#停止当前ES 节点
service elasticsearch stop
#卸载所有 ES 插件  进入/usr/share/elasticsearch目录
#elasticsearch-plugin list
#elasticsearch-plugin remove
# 卸载当前版本 ES
rpm -e elasticsearch
#重新安装 5.0.0 版本 ES
rpm install /usr/local/elasticsearch-5.0.0.rpm
#拷贝之前备份的配置文件
#启动 ES
sevice elasticsearch start
</code></pre><hr>
<h3 id="kibana">升级Kibana</h3>
<blockquote>
<p>重点在升级配置
手动（ansible 脚本）将旧的配置文件中的配置，替换到新版配置文件中的对应条目中，注意兼容</p>
</blockquote>
<pre><code>#停止Kibana
service kibana stop
#卸载旧版本
rpm -e kibana
#安装新版本 Kibana
rpm -ivh /usr/local/kibana-5.6.16-x86_64.rpm
</code></pre><h4 id="heading-13">升级配置</h4>
<pre><code>#启动Kibana
service kibana start 
</code></pre><h5 id="-ansible-">固化脚本（见 Ansible 脚本）</h5>
<pre><code>#停止Kibana
service kibana stop
#卸载旧版本
rpm -e kibana
#安装新版本
rpm -ivh XX.rpm
#升级配置

#启动Kibana
service kibana start
</code></pre><p>迁移 Kibana 数据 <br>
Kibana 数据存储在   Elasticsearch   .Kibana 索引中
比如下图中  生产环境 ELK 日志系统  有 4 个 kibana 实例，在 ES 集群中对应这有 4 个 .kibana 索引 
<img src="https://cdn.nlark.com/yuque/0/2019/png/330515/1576133404676-29e785e5-1c83-4b9f-a688-7f46e9563c59.png#align=left&amp;display=inline&amp;height=530&amp;name=image.png&amp;originHeight=1060&amp;originWidth=2470&amp;size=205099&amp;status=done&amp;style=none&amp;width=1235" alt="image.png">   </p>
<hr>
<h3 id="-filebeat">升级 Filebeat</h3>
<p>本次不升级 Filebeat</p>
<hr>
<h3 id="heading-14">升级的一些坑</h3>
<p>Kibana 偶现无法访问
升级 Kibana 的一个小坑，从 6.3 之后
server.rewriteBasePath 由默认的 false 改为了 true 这个值我们没配置过
所以在 Kibana 前面有反向代理的情况下 ，会有重写代理请求的可能
也就是我们访问 Kibana 没响应，但是Kibana服务其实还在</p>
<pre><code># This setting cannot end in a slash.
#server.basePath: &quot;&quot;
server.basePath: &quot;/anchang&quot;
# Specifies whether Kibana should rewrite requests that are prefixed with
# `server.basePath` or require that they are rewritten by your reverse proxy.
# This setting was effectively always `false` before Kibana 6.3 and will
# default to `true` starting in Kibana 7.0.
server.rewriteBasePath: false
</code></pre>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">ELK 问题排查</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/elk%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" />
            <id>https://example.com/posts/elk%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/</id>
            <updated>2020-04-04T23:58:56+08:00</updated>
            <published>2019-06-03T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[集群 Red 为了完整的查看索引，分片，主分片和副分片情况，所以可直接使用自带 Api 查看，输入地……]]></summary>
            
                <content type="html"><![CDATA[<h3 id="-red">集群 Red</h3>
<p>为了完整的查看索引，分片，主分片和副分片情况，所以可直接使用自带 Api 查看，输入地址:</p>
<pre><code>GET /_cat/shards?h=index,shard,prirep,state,unassigned.reason
</code></pre><p>  查询索引有问题的原因</p>
<pre><code>GET /_cluster/allocation/explain
</code></pre><p>官网文档
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-allocation-explain.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-allocation-explain.html</a></p>
<p>可以查看结果，线上 ES 是12个分片，3个备份，但是每个备份都会出现一个 UNASSIGNED 的情况，我们从最后的原因 INDEX CREATED 可得知是由于创建索引的API导致未分配。</p>
<pre><code>trackmodel-2019-01-02           8  r UNASSIGNED INDEX CREATED
trackmodel-2019-01-02           8  r STARTED 
trackmodel-2019-01-02           8  r STARTED 
trackmodel-2019-01-02           8  p STARTED 
trackmodel-2019-01-02           4  r STARTED 
trackmodel-2019-01-02           4  p STARTED 
trackmodel-2019-01-02           4  r STARTED 
trackmodel-2019-01-02           4  r UNASSIGNED INDEX CREATED
trackmodel-2019-01-02           7  r STARTED 
trackmodel-2019-01-02           7  p STARTED 
trackmodel-2019-01-02           7  r STARTED 
trackmodel-2019-01-02           7  r UNASSIGNED INDEX CREATED
.
.
.
trackmodel-2019-01-02           1  r UNASSIGNED INDEX CREATED
</code></pre><p>然后使用查看：
然后使用查看：</p>
<pre><code>GET /_cat/allocation?v
</code></pre><p>可以从从结果中得知其实线上 ES 只有3个 node，但是由上面可知，我在创建索引时定义了3个副本，所以就是 node 数少于分片副本数导致的。</p>
<pre><code>shards disk.indices disk.used disk.avail disk.total disk.percent host          ip            node
   204      120.4gb   135.6gb       61gb    196.7gb           68 172.16.41.127 172.16.41.127 trace-es-node-3
   204      121.9gb   139.3gb     57.4gb    196.7gb           70 172.16.41.125 172.16.41.125 trace-es-node-1
   204      120.5gb   136.9gb     59.7gb    196.7gb           69 172.16.41.126 172.16.41.126 trace-es-node-2
   204  unassinged
</code></pre><p>所以这个问题就很好解决了，由于本身我的索引是按每天日期新建，使用别名来进行查询，会定时删除过期索引，所以直接修改创建索引时代码，将创建副本数改为2即可解决问题。不需要更改之前出现的 UNASSIGNED 分片的问题，但是如果要更改的话可以直接使用以下命令来调整对应日期索引的副本数</p>
<pre><code>PUT /trackmodel-2019-01-02/_settings
{  
    &quot;number_of_replicas&quot; : 2 
}
</code></pre><h3 id="heading">问题探讨</h3>
<p>虽然这次的 UNASSIGNED 问题解决和问题原因非常简单，但是其实引起 UNASSIGNED 是有很多原因的，在 ES 开发中也经常会遇到这个问题，比如以下几种可能就都会引起 UNASSIGNED 的问题：</p>
<pre><code>1. Shard allocation  过程中的延迟机制
2. nodes 数小于分片副本数
3. 检查是否开启 cluster.routing.allocation.enable 参数
4. 分片的历史数据丢失了
5. 磁盘不够用了
6. es 的版本问题
</code></pre><p>这些不同问题导致的 UNASSIGNED 都需要不同的方式去解决，所以在本文第一条命令中最后的 reason 列就相对比较重要了，是知道问题原因的初步思路来源，大概分为以下几种</p>
<pre><code>1）INDEX_CREATED：由于创建索引的API导致未分配。
2）CLUSTER_RECOVERED ：由于完全集群恢复导致未分配。
3）INDEX_REOPENED ：由于打开open或关闭close一个索引导致未分配。
4）DANGLING_INDEX_IMPORTED ：由于导入dangling索引的结果导致未分配。
5）NEW_INDEX_RESTORED ：由于恢复到新索引导致未分配。
6）EXISTING_INDEX_RESTORED ：由于恢复到已关闭的索引导致未分配。
7）REPLICA_ADDED：由于显式添加副本分片导致未分配。
8）ALLOCATION_FAILED ：由于分片分配失败导致未分配。
9）NODE_LEFT ：由于承载该分片的节点离开集群导致未分配。
10）REINITIALIZED ：由于当分片从开始移动到初始化时导致未分配（例如，使用影子shadow副本分片）。
11）REROUTE_CANCELLED ：作为显式取消重新路由命令的结果取消分配。
12）REALLOCATED_REPLICA ：确定更好的副本位置被标定使用，导致现有的副本分配被取消，出现未分配。
</code></pre><p>设置单个索引在每个节点上最大分配分配数的设置是在索引模板 （index template）里设置</p>
<p>index.routing.allocation.total_shards_per_node&quot;:3</p>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">ElasticSearch 压测</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/elk%E5%8E%8B%E6%B5%8B/" />
            <id>https://example.com/posts/elk%E5%8E%8B%E6%B5%8B/</id>
            <updated>2020-04-04T23:59:01+08:00</updated>
            <published>2019-05-22T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[本次压测目标： 测试集群读写性能 &amp; 为集群容量规划提供数据支撑 测试 ES 新版本，结合实际场景……]]></summary>
            
                <content type="html"><![CDATA[<h4 id="heading">本次压测目标：</h4>
<ul>
<li>测试集群读写性能 &amp; 为集群容量规划提供数据支撑</li>
<li>测试 ES 新版本，结合实际场景与老版本进行比较，评估是否进行升级</li>
<li>性能优化&amp;性能问题诊断</li>
<li>对 ES 配置参数进行修改，评估优化效果（长期）</li>
<li>确定系统稳定性，考察系统功能极限与稳定性
<h4 id="heading-1"></h4>
</li>
</ul>
<h4 id="heading-2">可选择的压测工具</h4>
<ul>
<li>Elastic 官方压测工具：rally</li>
<li>基于HTTP 的压测第三方压测工具：JMeter 等</li>
</ul>
<h4 id="heading-3">压测流程如下</h4>
<ul>
<li>压测计划</li>
<li>脚本开发</li>
<li>压测环境搭建&amp;压测开始</li>
<li>结果分析</li>
</ul>
<hr>
<ul>
<li>为方便对比，压测数据统一使用 geonames 数据集（大小3.2G，共11523468个文档）</li>
<li>时间： 完整的一次压测过程，时间以小时计</li>
</ul>
<h4 id="heading-4">安装</h4>
<p>依赖</p>
<ul>
<li>3.5+ including pip3 </li>
<li>git 1.9+</li>
<li>JDK </li>
</ul>
<pre><code>pip3 install esrally
</code></pre><h4 id="elastic-httpselasticsearch-benchmarkselasticco">Elastic 官方压测结果<a href="https://elasticsearch-benchmarks.elastic.co/">链接</a></h4>
<pre><code>https://elasticsearch-benchmarks.elastic.co/
</code></pre><h4 id="heading-5">配置</h4>
<pre><code> esrally configure --advanced-config
</code></pre><p>只测试 1000 条数据</p>
<pre><code>esrally --distribution-version=5.6.16 --test-mode
</code></pre><p>开始测试各版本单机性能</p>
<pre><code>esrally race --distribution-version=5.6.16 --track=geonames --challenge=append-no-conflicts --user-tag=&quot;version:5.6.16&quot;
</code></pre><h4 id="heading-6"></h4>
<h4 id="-tracks">查看本地 tracks</h4>
<pre><code>esrally list tracks
</code></pre><h4 id="heading-7"></h4>
<h4 id="heading-8">压测现有集群</h4>
<pre><code>esrally race --pipeline=benchmark-only --target-hosts=10.0.19.218:9200 --distribution-version=6.7.2  --track=geonames --challenge=append-no-conflicts
</code></pre><h4 id="10019218">测试环境（10.0.19.218）先跑一遍</h4>
<pre><code>esrally race --pipeline=benchmark-only --target-hosts=10.0.19.218:9200 --distribution-version=6.7.2  --track=geonames --challenge=append-no-conflicts
</code></pre><h5 id="heading-9">结果</h5>
<pre><code>|                                                         Metric |                   Task |    Value |    Unit |
|---------------------------------------------------------------:|-----------------------:|---------:|--------:|
|                     Cumulative indexing time of primary shards |                        |  157.082 |     min |
|             Min cumulative indexing time across primary shards |                        |        0 |     min |
|          Median cumulative indexing time across primary shards |                        | 0.256483 |     min |
|             Max cumulative indexing time across primary shards |                        |  30.3196 |     min |
|            Cumulative indexing throttle time of primary shards |                        |        0 |     min |
|    Min cumulative indexing throttle time across primary shards |                        |        0 |     min |
| Median cumulative indexing throttle time across primary shards |                        |        0 |     min |
|    Max cumulative indexing throttle time across primary shards |                        |        0 |     min |
|                        Cumulative merge time of primary shards |                        |  161.364 |     min |
|                       Cumulative merge count of primary shards |                        |    23823 |         |
|                Min cumulative merge time across primary shards |                        |        0 |     min |
|             Median cumulative merge time across primary shards |                        |  1.43677 |     min |
|                Max cumulative merge time across primary shards |                        |  23.3528 |     min |
|               Cumulative merge throttle time of primary shards |                        | 0.933833 |     min |
|       Min cumulative merge throttle time across primary shards |                        |        0 |     min |
|    Median cumulative merge throttle time across primary shards |                        |        0 |     min |
|       Max cumulative merge throttle time across primary shards |                        |   0.1983 |     min |
|                      Cumulative refresh time of primary shards |                        |  59.8701 |     min |
|                     Cumulative refresh count of primary shards |                        |   254455 |         |
|              Min cumulative refresh time across primary shards |                        |        0 |     min |
|           Median cumulative refresh time across primary shards |                        |  1.47577 |     min |
|              Max cumulative refresh time across primary shards |                        |  7.86888 |     min |
|                        Cumulative flush time of primary shards |                        |  1.82032 |     min |
|                       Cumulative flush count of primary shards |                        |       76 |         |
|                Min cumulative flush time across primary shards |                        |        0 |     min |
|             Median cumulative flush time across primary shards |                        |  0.00315 |     min |
|                Max cumulative flush time across primary shards |                        | 0.473867 |     min |
|                                             Total Young Gen GC |                        |  156.258 |       s |
|                                               Total Old Gen GC |                        |    0.504 |       s |
|                                                     Store size |                        |  10.2503 |      GB |
|                                                  Translog size |                        |  3.40687 |      GB |
|                                         Heap used for segments |                        |  28.6233 |      MB |
|                                       Heap used for doc values |                        |  1.08705 |      MB |
|                                            Heap used for terms |                        |  25.1319 |      MB |
|                                            Heap used for norms |                        | 0.283142 |      MB |
|                                           Heap used for points |                        | 0.717692 |      MB |
|                                    Heap used for stored fields |                        |  1.40359 |      MB |
|                                                  Segment count |                        |      394 |         |
|                                                 Min Throughput |           index-append |  13680.2 |  docs/s |
|                                              Median Throughput |           index-append |  15924.4 |  docs/s |
|                                                 Max Throughput |           index-append |  17251.8 |  docs/s |
|                                        50th percentile latency |           index-append |  1917.34 |      ms |
|                                        90th percentile latency |           index-append |  3282.31 |      ms |
|                                        99th percentile latency |           index-append |  4744.88 |      ms |
|                                      99.9th percentile latency |           index-append |  6111.59 |      ms |
|                                       100th percentile latency |           index-append |   6868.8 |      ms |
|                                   50th percentile service time |           index-append |  1917.34 |      ms |
|                                   90th percentile service time |           index-append |  3282.31 |      ms |
|                                   99th percentile service time |           index-append |  4744.88 |      ms |
|                                 99.9th percentile service time |           index-append |  6111.59 |      ms |
|                                  100th percentile service time |           index-append |   6868.8 |      ms |
|                                                     error rate |           index-append |        0 |       % |
|                                                 Min Throughput |            index-stats |    47.27 |   ops/s |
|                                              Median Throughput |            index-stats |    47.77 |   ops/s |
|                                                 Max Throughput |            index-stats |    47.88 |   ops/s |
|                                        50th percentile latency |            index-stats |  9808.46 |      ms |
|                                        90th percentile latency |            index-stats |  13753.5 |      ms |
|                                        99th percentile latency |            index-stats |  14571.1 |      ms |
|                                      99.9th percentile latency |            index-stats |  14645.1 |      ms |
|                                       100th percentile latency |            index-stats |  14653.5 |      ms |
|                                   50th percentile service time |            index-stats |   20.069 |      ms |
|                                   90th percentile service time |            index-stats |  23.2235 |      ms |
|                                   99th percentile service time |            index-stats |  28.6327 |      ms |
|                                 99.9th percentile service time |            index-stats |  46.0017 |      ms |
|                                  100th percentile service time |            index-stats |  48.4586 |      ms |
|                                                     error rate |            index-stats |        0 |       % |
|                                                 Min Throughput |             node-stats |    51.61 |   ops/s |
|                                              Median Throughput |             node-stats |    54.58 |   ops/s |
|                                                 Max Throughput |             node-stats |    54.95 |   ops/s |
|                                        50th percentile latency |             node-stats |  4375.34 |      ms |
|                                        90th percentile latency |             node-stats |  7131.76 |      ms |
|                                        99th percentile latency |             node-stats |  7723.09 |      ms |
|                                      99.9th percentile latency |             node-stats |  7780.35 |      ms |
|                                       100th percentile latency |             node-stats |  7785.54 |      ms |
|                                   50th percentile service time |             node-stats |  17.4001 |      ms |
|                                   90th percentile service time |             node-stats |  21.1093 |      ms |
|                                   99th percentile service time |             node-stats |  23.9763 |      ms |
|                                 99.9th percentile service time |             node-stats |  34.4598 |      ms |
|                                  100th percentile service time |             node-stats |  37.7474 |      ms |
|                                                     error rate |             node-stats |        0 |       % |
|                                                 Min Throughput |                default |    49.87 |   ops/s |
|                                              Median Throughput |                default |    49.99 |   ops/s |
|                                                 Max Throughput |                default |    50.02 |   ops/s |
|                                        50th percentile latency |                default |  22.9061 |      ms |
|                                        90th percentile latency |                default |  35.2368 |      ms |
|                                        99th percentile latency |                default |  56.4199 |      ms |
|                                      99.9th percentile latency |                default |  60.2163 |      ms |
|                                       100th percentile latency |                default |  62.0121 |      ms |
|                                   50th percentile service time |                default |  21.0608 |      ms |
|                                   90th percentile service time |                default |  23.0526 |      ms |
|                                   99th percentile service time |                default |  24.0483 |      ms |
|                                 99.9th percentile service time |                default |   26.301 |      ms |
|                                  100th percentile service time |                default |  27.9663 |      ms |
|                                                     error rate |                default |        0 |       % |
|                                                 Min Throughput |                   term |   200.07 |   ops/s |
|                                              Median Throughput |                   term |   200.08 |   ops/s |
|                                                 Max Throughput |                   term |   200.14 |   ops/s |
|                                        50th percentile latency |                   term |  2.71801 |      ms |
|                                        90th percentile latency |                   term |  3.03285 |      ms |
|                                        99th percentile latency |                   term |  11.4885 |      ms |
|                                      99.9th percentile latency |                   term |  26.8219 |      ms |
|                                       100th percentile latency |                   term |  28.1828 |      ms |
|                                   50th percentile service time |                   term |  2.56728 |      ms |
|                                   90th percentile service time |                   term |  2.85599 |      ms |
|                                   99th percentile service time |                   term |  3.55974 |      ms |
|                                 99.9th percentile service time |                   term |  8.51489 |      ms |
|                                  100th percentile service time |                   term |  26.6701 |      ms |
|                                                     error rate |                   term |        0 |       % |
|                                                 Min Throughput |                 phrase |   199.27 |   ops/s |
|                                              Median Throughput |                 phrase |   200.03 |   ops/s |
|                                                 Max Throughput |                 phrase |   200.06 |   ops/s |
|                                        50th percentile latency |                 phrase |  4.25964 |      ms |
|                                        90th percentile latency |                 phrase |  13.8712 |      ms |
|                                        99th percentile latency |                 phrase |  35.8641 |      ms |
|                                      99.9th percentile latency |                 phrase |  39.0226 |      ms |
|                                       100th percentile latency |                 phrase |  39.8982 |      ms |
|                                   50th percentile service time |                 phrase |   4.0484 |      ms |
|                                   90th percentile service time |                 phrase |  4.60117 |      ms |
|                                   99th percentile service time |                 phrase |  8.69007 |      ms |
|                                 99.9th percentile service time |                 phrase |   33.045 |      ms |
|                                  100th percentile service time |                 phrase |  38.8927 |      ms |
|                                                     error rate |                 phrase |        0 |       % |
|                                                 Min Throughput |   country_agg_uncached |     2.35 |   ops/s |
|                                              Median Throughput |   country_agg_uncached |     2.36 |   ops/s |
|                                                 Max Throughput |   country_agg_uncached |     2.37 |   ops/s |
|                                        50th percentile latency |   country_agg_uncached |  43731.8 |      ms |
|                                        90th percentile latency |   country_agg_uncached |    50463 |      ms |
|                                        99th percentile latency |   country_agg_uncached |  52056.4 |      ms |
|                                       100th percentile latency |   country_agg_uncached |  52232.7 |      ms |
|                                   50th percentile service time |   country_agg_uncached |  407.051 |      ms |
|                                   90th percentile service time |   country_agg_uncached |  520.447 |      ms |
|                                   99th percentile service time |   country_agg_uncached |  588.956 |      ms |
|                                  100th percentile service time |   country_agg_uncached |  650.993 |      ms |
|                                                     error rate |   country_agg_uncached |        0 |       % |
|                                                 Min Throughput |     country_agg_cached |    99.69 |   ops/s |
|                                              Median Throughput |     country_agg_cached |   100.06 |   ops/s |
|                                                 Max Throughput |     country_agg_cached |   100.13 |   ops/s |
|                                        50th percentile latency |     country_agg_cached |  3.25071 |      ms |
|                                        90th percentile latency |     country_agg_cached |  3.58457 |      ms |
|                                        99th percentile latency |     country_agg_cached |  6.40293 |      ms |
|                                      99.9th percentile latency |     country_agg_cached |  32.8548 |      ms |
|                                       100th percentile latency |     country_agg_cached |  38.2042 |      ms |
|                                   50th percentile service time |     country_agg_cached |  3.09234 |      ms |
|                                   90th percentile service time |     country_agg_cached |  3.40055 |      ms |
|                                   99th percentile service time |     country_agg_cached |  4.56434 |      ms |
|                                 99.9th percentile service time |     country_agg_cached |  13.7336 |      ms |
|                                  100th percentile service time |     country_agg_cached |  38.0936 |      ms |
|                                                     error rate |     country_agg_cached |        0 |       % |
|                                                 Min Throughput |                 scroll |    20.02 | pages/s |
|                                              Median Throughput |                 scroll |    20.02 | pages/s |
|                                                 Max Throughput |                 scroll |    20.03 | pages/s |
|                                        50th percentile latency |                 scroll |  952.534 |      ms |
|                                        90th percentile latency |                 scroll |  977.789 |      ms |
|                                        99th percentile latency |                 scroll |  992.309 |      ms |
|                                       100th percentile latency |                 scroll |  997.499 |      ms |
|                                   50th percentile service time |                 scroll |   952.13 |      ms |
|                                   90th percentile service time |                 scroll |  977.355 |      ms |
|                                   99th percentile service time |                 scroll |  991.834 |      ms |
|                                  100th percentile service time |                 scroll |  996.983 |      ms |
|                                                     error rate |                 scroll |        0 |       % |
|                                                 Min Throughput |             expression |      0.9 |   ops/s |
|                                              Median Throughput |             expression |     0.91 |   ops/s |
|                                                 Max Throughput |             expression |     0.91 |   ops/s |
|                                        50th percentile latency |             expression |   150678 |      ms |
|                                        90th percentile latency |             expression |   176840 |      ms |
|                                        99th percentile latency |             expression |   182169 |      ms |
|                                       100th percentile latency |             expression |   182646 |      ms |
|                                   50th percentile service time |             expression |  1125.56 |      ms |
|                                   90th percentile service time |             expression |  1286.03 |      ms |
|                                   99th percentile service time |             expression |  1303.76 |      ms |
|                                  100th percentile service time |             expression |  1332.52 |      ms |
|                                                     error rate |             expression |        0 |       % |
|                                                 Min Throughput |        painless_static |     0.94 |   ops/s |
|                                              Median Throughput |        painless_static |     0.94 |   ops/s |
|                                                 Max Throughput |        painless_static |     0.94 |   ops/s |
|                                        50th percentile latency |        painless_static |   100312 |      ms |
|                                        90th percentile latency |        painless_static |   116157 |      ms |
|                                        99th percentile latency |        painless_static |   119962 |      ms |
|                                       100th percentile latency |        painless_static |   120267 |      ms |
|                                   50th percentile service time |        painless_static |  1051.41 |      ms |
|                                   90th percentile service time |        painless_static |  1220.43 |      ms |
|                                   99th percentile service time |        painless_static |  1354.25 |      ms |
|                                  100th percentile service time |        painless_static |  1364.53 |      ms |
|                                                     error rate |        painless_static |        0 |       % |
|                                                 Min Throughput |       painless_dynamic |     0.85 |   ops/s |
|                                              Median Throughput |       painless_dynamic |     0.85 |   ops/s |
|                                                 Max Throughput |       painless_dynamic |     0.85 |   ops/s |
|                                        50th percentile latency |       painless_dynamic |   127809 |      ms |
|                                        90th percentile latency |       painless_dynamic |   147685 |      ms |
|                                        99th percentile latency |       painless_dynamic |   152236 |      ms |
|                                       100th percentile latency |       painless_dynamic |   152736 |      ms |
|                                   50th percentile service time |       painless_dynamic |  1162.78 |      ms |
|                                   90th percentile service time |       painless_dynamic |  1314.62 |      ms |
|                                   99th percentile service time |       painless_dynamic |  1485.99 |      ms |
|                                  100th percentile service time |       painless_dynamic |  1552.55 |      ms |
|                                                     error rate |       painless_dynamic |        0 |       % |
|                                                 Min Throughput |            large_terms |     0.73 |   ops/s |
|                                              Median Throughput |            large_terms |     0.73 |   ops/s |
|                                                 Max Throughput |            large_terms |     0.73 |   ops/s |
|                                        50th percentile latency |            large_terms |   176892 |      ms |
|                                        90th percentile latency |            large_terms |   204470 |      ms |
|                                        99th percentile latency |            large_terms |   210667 |      ms |
|                                       100th percentile latency |            large_terms |   211348 |      ms |
|                                   50th percentile service time |            large_terms |  1359.82 |      ms |
|                                   90th percentile service time |            large_terms |   1438.7 |      ms |
|                                   99th percentile service time |            large_terms |  1510.96 |      ms |
|                                  100th percentile service time |            large_terms |  1545.09 |      ms |
|                                                     error rate |            large_terms |        0 |       % |
|                                                 Min Throughput |   large_filtered_terms |     0.73 |   ops/s |
|                                              Median Throughput |   large_filtered_terms |     0.73 |   ops/s |
|                                                 Max Throughput |   large_filtered_terms |     0.73 |   ops/s |
|                                        50th percentile latency |   large_filtered_terms |   176867 |      ms |
|                                        90th percentile latency |   large_filtered_terms |   204577 |      ms |
|                                        99th percentile latency |   large_filtered_terms |   210807 |      ms |
|                                       100th percentile latency |   large_filtered_terms |   211565 |      ms |
|                                   50th percentile service time |   large_filtered_terms |  1358.56 |      ms |
|                                   90th percentile service time |   large_filtered_terms |  1443.66 |      ms |
|                                   99th percentile service time |   large_filtered_terms |   1483.7 |      ms |
|                                  100th percentile service time |   large_filtered_terms |  1497.43 |      ms |
|                                                     error rate |   large_filtered_terms |        0 |       % |
|                                                 Min Throughput | large_prohibited_terms |     0.75 |   ops/s |
|                                              Median Throughput | large_prohibited_terms |     0.75 |   ops/s |
|                                                 Max Throughput | large_prohibited_terms |     0.75 |   ops/s |
|                                        50th percentile latency | large_prohibited_terms |   168425 |      ms |
|                                        90th percentile latency | large_prohibited_terms |   194882 |      ms |
|                                        99th percentile latency | large_prohibited_terms |   201165 |      ms |
|                                       100th percentile latency | large_prohibited_terms |   202136 |      ms |
|                                   50th percentile service time | large_prohibited_terms |  1339.58 |      ms |
|                                   90th percentile service time | large_prohibited_terms |  1425.45 |      ms |
|                                   99th percentile service time | large_prohibited_terms |  1568.95 |      ms |
|                                  100th percentile service time | large_prohibited_terms |  1646.36 |      ms |
|                                                     error rate | large_prohibited_terms |        0 |       % |


----------------------------------
[INFO] SUCCESS (took 3850 seconds)
----------------------------------
</code></pre><h4 id="heading-10">参考：</h4>
<p>[https://github.com/elastic/rally](</p>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">VScode使用笔记</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/vscode%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/" />
            <id>https://example.com/posts/vscode%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</id>
            <updated>2020-04-04T23:59:33+08:00</updated>
            <published>2019-01-14T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[电脑可能会换，系统可能会换，但是编辑器，可能会跟你很久. 推荐安装插件: Atom One Dark Theme #At……]]></summary>
            
                <content type="html"><![CDATA[<blockquote>
<p>电脑可能会换，系统可能会换，但是编辑器，可能会跟你很久.</p>
</blockquote>
<h5 id="heading">推荐安装插件:</h5>
<ol>
<li>Atom One Dark Theme   #Atom主题</li>
<li>Settings Sync         #配置同步</li>
<li>VSCode Great Icons    #文件图标替换</li>
</ol>
<h5 id="vscodepython">VScode写Python</h5>
<p>比如要自动格式化代码，只需要按下Alt+Shift+F，vscode就会调用autopep8自动格式化代码
然后就开始自动安装autopep8. autopep8在每个python环境中都要安装
如果是是用python-autopep8的插件 则要在终端中使用brew install autopep8</p>
<p>python虚拟环境:virtualenv</p>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Nginx实战</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" />
            <id>https://example.com/posts/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
            <updated>2020-04-04T23:59:23+08:00</updated>
            <published>2018-02-03T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[搭建一个简易的代理服务 nginx 常常用来作为代理服务器，这代表着服务器接收请求，然后将它们传……]]></summary>
            
                <content type="html"><![CDATA[<h1 id="heading">搭建一个简易的代理服务</h1>
<p>nginx 常常用来作为代理服务器，这代表着服务器接收请求，然后将它们传递给被代理服务器，得到请求的响应，再将它们发送给客户端。 我们将配置一个基本的代理服务器，它会处理本地图片文件的请求并返回其他的请求给被代理的服务器。在这个例子中，两个服务器都会定义在一个 nginx 实例中。 首先，通过在 nginx 配置文件中添加另一个 server 区块，来定义一个被代理的服务器，像下面的配置：</p>
<pre><code>server {
    listen 8080;
    root /data/up1;

    location / {
    }
}
</code></pre><!-- more --> 
<p>上面就是一个简单的服务器，它监听在 8080 端口（之前，listen 并没被定义，是因为默认监听的 80 端口）并且会映射所有的请求给 本地文件目录 /data/up1。创建该目录，然后添加 index.html 文件。注意，root 指令是放在 server 上下文中。当响应请求的 location 区块中，没有自己的 root 指令，上述的 root 指令才会被使用。 接着，使用前面章节中的 server 配置，然后将它改为一个代理服务配置。在第一个 location 区块中，放置已经添加被代理服务器的协议，名字和端口等参数的 proxy_pass 指令（在这里，就是 http://localhost:8080）:</p>
<pre><code>server {
    location / {
        proxy_pass http://localhost:8080;
    }

    location /images/ {
        root /data;
    }
}
</code></pre><h2 id="location">location相关</h2>
<p>我们将修改第二个 location 区块，使他返回一些典型后缀的图片文件请求，
现在它只会映射带有 /images/ 前缀的请求到 /data/images 目录下。
修改后的 location 指令如下：</p>
<pre><code>location ~ \.(gif|jpg|png)$ {
    root /data/images;
}
</code></pre><p>该参数是一个正则表达式，它会匹配所有以 .gif，.jpg 或者 .png 结尾的 URIs。
一个正则表达式需要以 ~ 开头。匹配到的请求会被映射到 /data/images 目录下。
当 nginx 在选择 location 去响应一个请求时，它会先检测带有前缀的 location 指令，记住先是检测带有最长前缀的 location，然后检测正则表达式。
如果有一个正则的匹配的规则，nginx 会选择该 location，否则，会选择之前缓存的规则。 最终，一个代理服务器的配置结果如下：</p>
<pre><code> server {
    location / {
        proxy_pass http://localhost:8080/;
    }

    location ~ \.(gif|jpg|png)$ {
        root /data/images;
    }
}
</code></pre><p>该服务器会选择以 .gif，.jpg，或者 .png 结束的请求并且映射到 /data/images 目录（通过添加 URI 给 root 指令的参数），
接着将其他所有的请求映射到上述被代理的服务器。 为了使用新的配置，像前几个章节描述的一样，需要向 nginx 发送重载信号。
这还有很多其他的指令，可以用于进一步配置代理连接。</p>
<h3 id="location-">location 配置</h3>
<ol>
<li>= 开头表示精确匹配</li>
<li>^~ 开头表示uri以某个常规字符串开头，不是正则匹配</li>
<li>~ 开头表示区分大小写的正则匹配;</li>
<li>~* 开头表示不区分大小写的正则匹配</li>
<li>/ 通用匹配, 如果没有其它匹配,任何请求都会匹配到</li>
</ol>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Centos更换yum源</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/%E6%9B%B4%E6%8D%A2yum%E6%BA%90/" />
            <id>https://example.com/posts/%E6%9B%B4%E6%8D%A2yum%E6%BA%90/</id>
            <updated>2020-04-04T23:58:33+08:00</updated>
            <published>2018-02-02T11:12:48+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[第一步：备份你的原镜像文件，以免出错后可以恢复。 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 第二步：下载新的CentOS-……]]></summary>
            
                <content type="html"><![CDATA[<h4 id="heading">第一步：备份你的原镜像文件，以免出错后可以恢复。</h4>
<pre><code>mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
</code></pre><h4 id="centos-baserepo-etcyumreposd">第二步：下载新的CentOS-Base.repo 到/etc/yum.repos.d/</h4>
<pre><code>CentOS 6
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo
CentOS 7
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo

</code></pre><!-- more --> 
<h4 id="yum-makecache">第三步：运行yum makecache生成缓存</h4>
<pre><code>yum clean all
yum makecache
</code></pre>]]></content>
            
            
            
            
            
                
                    
                        
                            
                            
                            
                                <category scheme="https://example.com/categories/life/" term="Life" label="Life" />
                            
                        
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Zabbix4.4安装</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/zabbix4.4-%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3/" />
            <id>https://example.com/posts/zabbix4.4-%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3/</id>
            <updated>2020-04-04T23:59:40+08:00</updated>
            <published>2017-11-28T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[Zabbix介绍 Zabbix 是一个企业级的分布式开源监控方案。 zabbix server ，zabbix agent&gt; 与可选组件z……]]></summary>
            
                <content type="html"><![CDATA[<h3 id="zabbix">Zabbix介绍</h3>
<blockquote>
<p>Zabbix 是一个企业级的分布式开源监控方案。
zabbix server ，zabbix agent&gt; 与可选组件zabbix proxy。</p>
</blockquote>
<p><strong>API 功能 ：</strong> 应用api功能，可以方便的和其他系统结合，包括手机客户端的使用。</p>
<p>更多功能请查看<a href="http://www.zabbix.com/documentation.php">http://www.zabbix.com/documentation.php</a></p>
<h3 id="zabbix-1">安装zabbix环境及准备工作</h3>
<p>zabbix 依赖 LAMP （LNMP 也可以）</p>
<h5 id="zabbix-2">安装zabbix</h5>
<ul>
<li>（单机）--&gt; LAMP</li>
<li>（架构）--&gt; LAP + MYSQL（生产环境，建议数据库与 Zabbix-Server 分开部署）</li>
<li>服务端端口：10051</li>
<li>客户端端口：10050</li>
</ul>
<hr>
<p>1，安装Zabbix需要的硬件环境及软件版本，我这里在官网上查了一下，你可以根据自己的环境和要求来选择：<br />下表是几个硬件配置的示例:</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>平台</th>
<th>**CPU/**内存</th>
<th>数据库</th>
<th>监控主机数量</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>小型</strong></td>
<td>CentOS</td>
<td>虚拟应用</td>
<td>MySQL   InnoDB</td>
<td>100</td>
</tr>
<tr>
<td><strong>中型</strong></td>
<td>CentOS</td>
<td>2 CPU   cores/2GB</td>
<td>MySQL   InnoDB</td>
<td>500</td>
</tr>
<tr>
<td><strong>大型</strong></td>
<td>RedHat   Enterprise Linux</td>
<td>4 CPU   cores/8GB</td>
<td>RAID10   MySQL InnoDB or PostgreSQL</td>
<td>&gt;1000</td>
</tr>
<tr>
<td><strong>巨大型</strong></td>
<td>RedHat   Enterprise Linux</td>
<td>8 CPU   cores/16GB</td>
<td>快速RAID10 MySQL InnoDB or PostgreSQL</td>
<td>&gt;10000</td>
</tr>
</tbody>
</table>
<p>具体的配置极其依赖于Active Item数量和轮询频率。如需要进行大规模部署，强烈建议将数据库进行独立部署。</p>
<hr>
<p>2，接下来我说一下我实验环境</p>
<table>
<thead>
<tr>
<th>操作系统</th>
<th>主机IP</th>
<th>主机名称</th>
<th>安装软件</th>
<th>安装zabbix版本</th>
<th>MySQL版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>Centos7.3</td>
<td>192.68.0.20</td>
<td>zabbix</td>
<td>Zabbix-server</td>
<td>Zabbix 3.4.10</td>
<td>MySQL5.7.22</td>
</tr>
<tr>
<td>centos6.5</td>
<td>192.168.0.157</td>
<td>Test02</td>
<td>zabbix-agent</td>
<td>zabbix-agent-3.4.10</td>
<td>/</td>
</tr>
</tbody>
</table>
<h3 id="zabbix-3">安装zabbix</h3>
<p>3.1）在监控主机上需要预先安装yum 源，下面正式开始安装；</p>
<pre><code>rpm -Uvh https://repo.zabbix.com/zabbix/4.4/rhel/6/x86_64/zabbix-release-4.4-1.el6.noarch.rpm
yum clean all
</code></pre><p>3.2）安装Zabbix-server包和zabbix-agent包</p>
<pre><code>yum -y install zabbix-server-mysql zabbix-web-mysql zabbix-agent
</code></pre><p>3.3）下载安装mysql源</p>
<pre><code>rpm -ivh https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm
</code></pre><p>3.4)查看当前可用的Mysql安装源</p>
<pre><code>[root@zabbix ~]# yum repolist enabled | grep &quot;mysql.*-community.*&quot;
mysql-connectors-community/x86_64 MySQL Connectors Community                  51
mysql-tools-community/x86_64      MySQL Tools Community                       63
mysql57-community/x86_64          MySQL 5.7 Community Server                 267
</code></pre><h3 id="mysql-">MySQL 安装&amp;配置</h3>
<pre><code>yum -y install mysql-community-server
</code></pre><p>3.6）启动mysql服务并设置开机启动</p>
<pre><code>[root@zabbix ~]#systemctl start mysqld
[root@zabbix ~]#systemctl enable mysqld
</code></pre><p>3.7)进入MySQL并修改密码</p>
<pre><code>[root@zabbix ~]# cat /var/log/mysqld.log | grep password
或者：/usr/bin/mysqladmin -u root password 'new-password'

[root@zabbix ~]# mysql -uroot -pnew-password
mysql&gt; SET PASSWORD = PASSWORD ('Pass123!');
如果想用简单的密码必须先改一个变量；
mysql&gt; set global validate_password_policy=0;
mysql&gt; SET PASSWORD = PASSWORD ('12345678');
不然你改密码会不通过，会有密码复杂度要求。
</code></pre><p>3.8）创建数据库和zabbix用户并授权</p>
<pre><code>mysql&gt; create database zabbix character set utf8 collate utf8_bin;
Query OK, 1 row affected (10.03 sec)
mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by 'Pass123!';
Query OK, 0 rows affected, 1 warning (0.00 sec)
</code></pre><p>3.9)导入初始架构（Schema）和数据</p>
<pre><code>[root@zabbix ~]#cd zabbix-server-mysql-4.4.1/
[root@zabbix  zabbix-server-mysql-3.4.10 ~]#zcat create.sql.gz | mysql -uzabbix -pPass123! -D zabbix
mysql: [Warning] Using a password on the command line interface can be insecure.
</code></pre><p>3.10)然后进入mysql查看这些内容是否导入进去</p>
<pre><code>mysql&gt; show tables from zabbix;
</code></pre><p><img src="https://blog-pic-1253367462.cos.ap-shanghai.myqcloud.com/1574069473892-5a56476b-3d7f-46f5-b559-02bb58924e09.png" alt="image.png"></p>
<pre><code>mysql&gt; select count(*) tables,table_schema from information_schema.tables where table_schema =&quot;zabbix&quot;;
</code></pre><hr>
<p>4.修改配置文件，给服务授权、启动Zabbix Server服务<br />4.1）修改配置文件</p>
<pre><code>备注：记得先备份 cp /etc/zabbix/zabbix_server.conf  /etc/zabbix/zabbix_server.conf.bak 
[root@zabbix ~]#vim  /etc/zabbix/zabbix_server.conf
DBHost=localhost
DBName=zabbix
DBUser=zabbix
DBPassword=Pass123!
</code></pre><p>4.2）给服务授权</p>
<pre><code>[root@zabbix ~]#chown -R zabbix:zabbix /etc/zabbix/
[root@zabbix ~]#chmod -R 755 /etc/zabbix/
</code></pre><h5 id="zabbix-server">启动Zabbix Server服务</h5>
<pre><code>[root@zabbix ~]#systemctl start  zabbix-server
[root@zabbix ~]#systemctl enable zabbix-server
</code></pre><p>备注：这里会有一个坑，就是在启动zabbix服务会失败，Job for zabbix-server.service failed. See 'systemctl status zabbix-server.service' and 'journalctl -xn' for details.查了一下原因是gnutls-3.3的高版本问题，解决办法是;1,先卸载这个高版本的gnutls-3.3,命令：rpm -e gnutls-3.3.24-1.el7.x86_64 --nodeps2，然后去网上下载一个gnutls-3.1的版本，然后使用命令rpm -Uvh --force gnutls-3.1.18-8.el7.x86_64.rpm</p>
<hr>
<p><a name="13601554"></a></p>
<h5 id="zabbixphp">编辑Zabbix前端的PHP配置</h5>
<ul>
<li>Zabbix前端的Apache配置文件位于 /etc/httpd/conf.d/zabbix.conf 。一些PHP设置已经完成了配置。</li>
</ul>
<pre><code>#CentOS6 中获取zabbix apache 配置路径命令
rpm -ql zabbix-web | grep example.conf
</code></pre><pre><code>[root@zabbix ~]# vim /etc/httpd/conf.d/zabbix.conf
找到&lt;IfModule mod_php5.c&gt;标签下面
添加一条
php_value date.timezone Asia/Shanghai
</code></pre><ul>
<li>启动apache服务，并设置开机自启</li>
</ul>
<pre><code>[root@zabbix ~]#systemctl start httpd
[root@zabbix ~]#systemctl enable  httpd
</code></pre><hr>
<p><a name="2bbf3759"></a></p>
<h4 id="zabbix-web-">Zabbix Web 页面配置</h4>
<p>1，访问ip：<a href="http://192.168.0.20/zabbix/index.php">http://IP/zabbix/index.php</a><br /></p>
<p>中间省略一部分-----------------------------直接到登录界面了。<br /></p>
<p><img src="https://blog-pic-1253367462.cos.ap-shanghai.myqcloud.com/image-20200324220001544.png" alt="">默认的用户名是：Admin 密码：zabbix<br /></p>
<h4 id="zabbix-agent">Zabbix Agent</h4>
<ul>
<li>添加一台Linux客户端机器（ip:192.168.0.157）</li>
</ul>
<p>访问zabbix官网：<a href="https://www.zabbix.com/download?zabbix=3.4&amp;os_distribution=centos&amp;os_version=6&amp;db=MySQL">https://www.zabbix.com/download?zabbix=3.4&amp;os_distribution=centos&amp;os_version=6&amp;db=MySQL</a></p>
<ul>
<li>添加Yum源:</li>
</ul>
<pre><code>#Centos6
rpm -ivh  http://repo.zabbix.com/zabbix/3.4/rhel/6/x86_64/zabbix-release-3.4-1.el6.noarch.rpm
#Centos7 
rpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-agent-3.4.1-1.el7.x86_64.rpm
</code></pre><ul>
<li>安装客户端agent软件</li>
</ul>
<pre><code>yum -y install zabbix-agent
</code></pre><ul>
<li>修改agent配置文件</li>
</ul>
<pre><code>grep -v '^$' /etc/zabbix/zabbix_agentd.conf |grep -v '^#'

PidFile=/var/run/zabbix/zabbix_agentd.pid
LogFile=/var/log/zabbix/zabbix_agentd.log
LogFileSize=0
Server=192.168.2.82
ServerActive=192.168.2.82:10050
Hostname=Test02
Include=/etc/zabbix/zabbix_agentd.d/*.conf
</code></pre><pre><code># 给配置文件授权
chmod 775 /etc/zabbix/zabbix_agentd.conf
# 启动agent服务并查看服务启动成功没有
/etc/init.d/zabbix-agent start 
netstat -lntup |grep zabbix_agent
</code></pre><ul>
<li>
<p>Web 页面添加第一台主机</p>
<ul>
<li>
<p>配置--主机---创建主机</p>
<p><img src="https://blog-pic-1253367462.cos.ap-shanghai.myqcloud.com/image-20200324215908603.png" alt="image-20200324215908603"></p>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>添加主机详细信息</li>
</ul>
<p><img src="https://blog-pic-1253367462.cos.ap-shanghai.myqcloud.com/image-20200324215844994.png" alt="image-20200324215844994"></p>
<h5 id="heading">添加主机模板信息</h5>
<p><img src="https://blog-pic-1253367462.cos.ap-shanghai.myqcloud.com/image-20200324215828877.png" alt="image-20200324215828877"></p>
<p>这样一台客户端Linux基本添加完成，过几分钟就能开到Zabbix图标变绿证明添加成功了。</p>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Gerrit CodeReview代码审核工具搭建配置</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/gerrit-codereview%E5%B7%A5%E5%85%B7%E6%90%AD%E5%BB%BA%E9%85%8D%E7%BD%AE/" />
            <id>https://example.com/posts/gerrit-codereview%E5%B7%A5%E5%85%B7%E6%90%AD%E5%BB%BA%E9%85%8D%E7%BD%AE/</id>
            <updated>2020-04-04T23:59:41+08:00</updated>
            <published>0001-01-01T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[一、环境准备 OS:centos6.8 gerrit依赖 (java&amp;git) yum -y install java-1.8.0 安装git 2.14.1 (低版本git同步gitlab时……]]></summary>
            
                <content type="html"><![CDATA[<h3 id="heading">一、环境准备</h3>
<ol>
<li>
<p>OS:centos6.8
gerrit依赖 (java&amp;git)
yum -y install java-1.8.0
安装git 2.14.1 (低版本git同步gitlab时会出问题)
yum install -y  <a href="http://opensource.wandisco.com/centos/6/git/x86_64/wandisco-git-release-6-1.noarch.rpm">http://opensource.wandisco.com/centos/6/git/x86_64/wandisco-git-release-6-1.noarch.rpm</a>
yum install -y git
yum -y install epel-release &amp;&amp; yum -y install nginx httpd-tools</p>
</li>
<li>
<p>gerrit环境
下载：Gerrit 2.15.1  wget <a href="https://www.gerritcodereview.com/download/gerrit-2.15.1.war">https://www.gerritcodereview.com/download/gerrit-2.15.1.war</a></p>
</li>
</ol>
<!-- more --> 
<ol start="3">
<li>gerrit管理帐号(可选，使用独立账号配置gerrit)
gerrit依赖，用来管理gerrit。
sudo adduser gerrit
sudo passwd gerrit
并将gerrit加入sudo权限
sudo vi /etc/sudoers
gerrit  ALL=(ALL:ALL) ALL
备注：这里我直接使用root用户安装</li>
</ol>
<h3 id="-gerrit">二. 安装与配置gerrit</h3>
<ol>
<li>配置gerrit
默认安装：java -jar gerrit-2.15.1.war init -d /usr/local/review(此目录最好放在空间比较大的目录)</li>
</ol>
<blockquote>
<p>配置的时候 该选择Y的时候要选择Y</p>
</blockquote>
<p>Install Verified label         [y/N]? N    //如果选择默认的话 会出现没有submit按钮
Install plugin download-commands version v2.13.4 [y/N]? y  //如果不安装 会出现创建项目没有clone地址</p>
<p>更新配置文件：vim /usr/local/review/etc/gerrit.config</p>
<pre><code>[gerrit]
        basePath = git
        serverId = 90b8d687-f735-4a2c-b117-0518fe571670
        canonicalWebUrl = http://192.168.2.238:8080/
[database]
        type = h2
        database = /usr/local/review/db/ReviewDB
[index]
        type = LUCENE
[auth]
        type = HTTP
[receive]
        enableSignedPush = true
[sendemail]
        enable = true
        smtpServer = smtp.exmail.qq.com
        smtpServerPort = 465  
        smtpEncryption = ssl
        smtpUser = *@*.com
        smtpPass = ***
        sslVerify = false
        from = Code Review &lt;*@*.comm&gt;
[container]
        user = root
        javaHome = /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-3.b10.el6_9.x86_64/jre
[sshd]
        listenAddress = *:29418
[httpd]
        listenUrl = http://192.168.2.238:8080/
[cache]
        directory = cache
</code></pre><ol start="2">
<li>配置nginx反向代理
配置：vim gerrit.conf</li>
</ol>
<pre><code>server {
         listen 80;
         server_name 192.168.2.238;
         auth_basic &quot;Welcomme to Gerrit Code Review Site!&quot;;
         auth_basic_user_file /opt/.passwd;
         location / {
            proxy_pass  http://192.168.2.238:8080;
         }
       }
</code></pre><ol start="3">
<li>
<p>配置gerrit账户密码 创建认证用户(通过httpd-tools工具)
touch /opt/.passwd
htpasswd -c -b /opt/.passwd admin 123456(管理员)</p>
</li>
<li>
<p>启动gerrit&amp;启动nginx
sudo /usr/local/review/bin/gerrit.sh restart
service nginx start</p>
</li>
<li>
<p>访问gerrit 管理界面 <a href="http://xxx.xxxxx.com/">http://xxx.xxxxx.com/</a>
第一次访问，需要输入第3步设置的admin及密码，该账户将作为gerrit管理员账户。</p>
</li>
</ol>
<h3 id="gerrit">三、如何使用gerrit</h3>
<p>前提：需要git使用端 / gerrit服务端配合使用。</p>
<ol>
<li>添加项目(gerrit 服务端)
1.1 使用gerrit添加新项目：（适用于开启新项目并使用gerrit）
使用gerrit管理界面创建项目</li>
</ol>
<p>1.2使用gerrit添加已有项目：（适用于已有项目下移植到gerrit中）
ssh -p 3389 gerrit1@公网IP gerrit create-project --name exist-project #建议采用管理界面添加
或者使用gerrit管理界面
然后将已有项目与gerrit上建立的exist-project关联，即将已有代码库代码push到gerrit中进行管理。
cd /usr/local/review/git/exist-project
git push ssh://gerrit1@公网IP:3389/exist-project <em>:</em></p>
<ol start="2">
<li>生成sshkey(git使用端)
在开发账户中生成sshkey，用作与gerrit服务器连接。
ssh-keygen -t rsa #生成sshkey</li>
</ol>
<p>cat ~/.ssh/id_rsa.pub #查看sshkey</p>
<ol start="3">
<li>
<p>添加sshkey到gerrit服务器(gerrit 服务端)
此步骤与git流程类似，即将/root/.ssh/id_rsa.pub内容添加到gerri和gitlab(后面gerrit和gitlab直接同步)</p>
</li>
<li>
<p>拉取代码＆配置git hooks(git client端)
验证sshkey是否配置成功：ssh -p 29418 <a href="mailto:admin@192.168.199.222">admin@192.168.199.222</a></p>
</li>
</ol>
<p>拉取代码url：选择 clone with commit-msg hook  //会直接配置Change-ID gerrit流程必备</p>
<p>修改代码并提交，推送时与原有git流程不一致，采用 git push origin HEAD:refs/for/master</p>
<h3 id="gerrit-websitecode-review">四.使用gerrit website完成code review</h3>
<p>当完成push后，可在gerrit管理界面看到当前提交code review的change。
查看需要code review的提交：
查看某次提交的详细信息（审核者+2可通过本次提交，提交者可通过Abandon本次提交）：
如果审核者+2通过后，可提交该次commit.</p>
<h5 id="gerritgitlab">配置gerrit和gitlab同步</h5>
<p>gerrit和gitlab同步需要配置两个地方 (前提示已将sshkey 添加至gerrit和gitlab)</p>
<p>配置 /root/.ssh/config</p>
<pre><code>Host 192.168.2.244
    User root
    IdentityFile ~/.ssh/id_rsa
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
    PreferredAuthentications publickey
</code></pre><p>配置replation插件
/usr/local/review/etc/replication.config   //以实际gerrit安装位置为准</p>
<pre><code>[remote &quot;192.168.2.244&quot;]
         url = git@192.168.2.244:root/${name}.git
         push = +refs/heads/*:refs/heads/*
         push = +refs/tags/*:refs/tags/*
         push = +refs/changes/*:refs/changes/*
         timtout = 30
         threads = 3
</code></pre><h3 id="gerrit-1">五.gerrit注意事项</h3>
<ul>
<li>需要为每个使用者分配gerrit账号，不要都使用admin账号，因为admin账号可直接push master</li>
<li>push代码时需要使用
git push origin HEAD:refs/for/master(branch),gerrit默认关闭非admin账号的push direct权限</li>
<li>push代码时需要commit email与gerrit logoutaccount email一致，否则无法push成功，可选择关闭email notify，并开启forge user权限，或者通过修改gerrit数据库account email信息</li>
<li>git配置的email一定要和gerrit里注册的email一致，否者push会出错</li>
<li>gerrit数据库与gitlab同步，需要安装replication插件，并开启该功能</li>
</ul>
]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Yum 命令被锁</title>
            <link rel="alternate" type="text/html" href="https://example.com/posts/yum%E5%91%BD%E4%BB%A4%E8%A2%AB%E9%94%81%E8%A7%A3%E5%86%B3/" />
            <id>https://example.com/posts/yum%E5%91%BD%E4%BB%A4%E8%A2%AB%E9%94%81%E8%A7%A3%E5%86%B3/</id>
            <updated>2020-04-04T23:59:38+08:00</updated>
            <published>0001-01-01T00:00:00+00:00</published>
            <author>
                    <name>陈欣宇</name>
                    <uri>https://nevea.top/</uri>
                    <email>hichenxinyu@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)</rights>
    
    <summary type="html"><![CDATA[root@centos001 ~]# yum clean all //执行一个yum命令 已加载插件：fastestmirror Repodata is over 2 weeks old. Install……]]></summary>
            
                <content type="html"><![CDATA[<pre><code>root@centos001 ~]# yum clean all      //执行一个yum命令
已加载插件：fastestmirror
Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast
/var/run/yum.pid 已被锁定，PID 为 2308 的另一个程序正在运行。  //可以看到文件被锁定

</code></pre><!-- more --> 
<h4 id="yum">此处发现命令被锁，无法使用新的yum命令</h4>
<p>解决方法</p>
<ol>
<li>先用CTRL+Z暂停命令</li>
<li>输入命令：</li>
</ol>
<pre><code>rm -f /var/run/yum.pid    //删除这个文件
</code></pre><p>3 在执行yum命令就ok了</p>
<pre><code>[root@centos001 ~]# yum clean all
已加载插件：fastestmirror
正在清理软件源： base extras updates
Cleaning up everything
</code></pre>]]></content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
</feed>
